{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os, csv, sys\n",
    "import pyfasttext as ft\n",
    "import multiprocessing\n",
    "import nltk\n",
    "from nltk.tokenize import WordPunctTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the existing datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Discovery dataset is distributed using InferSent compatible format;\n",
    "\n",
    "You can use the following cells in order to convert it to csv/tsv with desired format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/data/name/Discovery/data/\" # replace with repository data path\n",
    "data_path = \"/data/sileo/libs/Discovery/data/\"\n",
    "dataset_path = os.path.join(data_path,\"DiscoveryBase\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>y</th>\n",
       "      <th>set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>He  helped  to  found  the  Mexican  American ...</td>\n",
       "      <td>Sanchez  became  involved  with  the  American...</td>\n",
       "      <td>subsequently,</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Then  click  the  ``  Paper  Clip  ''  button ...</td>\n",
       "      <td>You  can  use  any  handheld  device  that  ru...</td>\n",
       "      <td>alternately,</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>That  's  a  long  way  for  a  program  that ...</td>\n",
       "      <td>FAU  is  a  program  that  under  Cooney  and ...</td>\n",
       "      <td>presently,</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FORT  DRUM  ,  N.Y.  -  Throughout  its  histo...</td>\n",
       "      <td>Soldiers  receive  assignments  to  multiple  ...</td>\n",
       "      <td>typically,</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>They  continued  to  dig  noting  that  there ...</td>\n",
       "      <td>Every  ten  feet  they  found  a  layer  of  l...</td>\n",
       "      <td>curiously,</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  s1  \\\n",
       "0  He  helped  to  found  the  Mexican  American ...   \n",
       "1  Then  click  the  ``  Paper  Clip  ''  button ...   \n",
       "2  That  's  a  long  way  for  a  program  that ...   \n",
       "3  FORT  DRUM  ,  N.Y.  -  Throughout  its  histo...   \n",
       "4  They  continued  to  dig  noting  that  there ...   \n",
       "\n",
       "                                                  s2              y    set  \n",
       "0  Sanchez  became  involved  with  the  American...  subsequently,  train  \n",
       "1  You  can  use  any  handheld  device  that  ru...   alternately,  train  \n",
       "2  FAU  is  a  program  that  under  Cooney  and ...     presently,  train  \n",
       "3  Soldiers  receive  assignments  to  multiple  ...     typically,  train  \n",
       "4  Every  ten  feet  they  found  a  layer  of  l...     curiously,  train  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_df=[]\n",
    "for cv in [\"train\",\"test\",\"dev\"]:\n",
    "    df_cv=pd.DataFrame(list(zip(\n",
    "        open(f\"{dataset_path}/s1.{cv}\").read().splitlines(),\n",
    "        open(f\"{dataset_path}/s2.{cv}\").read().splitlines(),\n",
    "        open(f\"{dataset_path}/labels.{cv}\").read().splitlines()))\n",
    "        ,columns=[\"s1\",\"s2\",\"y\"])\n",
    "    df_cv[\"set\"]=cv\n",
    "    l_df+=[df_cv]\n",
    "    \n",
    "df_discovery = pd.concat(l_df)\n",
    "del l_df, df_cv\n",
    "df_discovery.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export in another format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/data/name/Discovery/data/DiscoveryBig/\"  # replace with desired output path\n",
    "data_dir = \"/data/sileo/libs/Discovery/data/DiscoveryBig/\"\n",
    "\n",
    "for cv in [\"train\",\"dev\",\"test\"]:\n",
    "    df_discovery[df_discovery.set==cv].to_csv(data_dir+f\"/{cv}.tsv\", index=False, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Processor\n",
    "To be used with BERT Huggingface package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
    "\n",
    "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
    "        \"\"\"Constructs a InputExample.\n",
    "        Args:\n",
    "            guid: Unique id for the example.\n",
    "            text_a: string. The untokenized text of the first sequence. For single\n",
    "            sequence tasks, only this sequence must be specified.\n",
    "            text_b: (Optional) string. The untokenized text of the second sequence.\n",
    "            Only must be specified for sequence pair tasks.\n",
    "            label: (Optional) string. The label of the example. This should be\n",
    "            specified for train and dev examples, but not for test examples.\n",
    "        \"\"\"\n",
    "        self.guid = guid\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "        self.label = label\n",
    "        \n",
    "class DataProcessor(object):\n",
    "    \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"Gets the list of labels for this data set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @classmethod\n",
    "    def _read_tsv(cls, input_file, quotechar=None):\n",
    "        \"\"\"Reads a tab separated value file.\"\"\"\n",
    "        with open(input_file, \"r\") as f:\n",
    "            reader = csv.reader(f, delimiter=\"\\t\", quotechar=quotechar)\n",
    "            lines = []\n",
    "            for line in reader:\n",
    "                if sys.version_info[0] == 2:\n",
    "                    line = list(unicode(cell, 'utf-8') for cell in line)\n",
    "                lines.append(line)\n",
    "            return lines\n",
    "        \n",
    "class DiscoveryProcessor(DataProcessor):\n",
    "    \"\"\"Processor for the Discovery data set.\"\"\"\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        logger.info(\"LOOKING AT {}\".format(os.path.join(data_dir, \"train.tsv\")))\n",
    "        return self._create_examples(\n",
    "            self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(\n",
    "            self._read_tsv(os.path.join(data_dir, \"dev.tsv\")), \"dev\")\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return list(disc_label.keys())\n",
    "\n",
    "    def _create_examples(self, lines, set_type):\n",
    "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
    "        examples = []\n",
    "        for (i, line) in enumerate(lines):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            guid = \"%s-%s\" % (set_type, i)\n",
    "            text_a = line[1].lower()\n",
    "            text_b = line[0].lower()\n",
    "            label = line[2]\n",
    "            \n",
    "            examples.append(\n",
    "                InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n",
    "        return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what  you  are  asking  them  to  do  costs  tons  of  money  . you  have  no  idea  how  much  work  goes  into  outreach  for  a  school  . first,\n"
     ]
    }
   ],
   "source": [
    "disc = DiscoveryProcessor()\n",
    "x = disc.get_dev_examples(data_dir)[0]\n",
    "print(x.text_a, x.text_b, x.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mining markers on a new corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove low quality sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download lid from https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin\n",
    "\n",
    "path_to_lid = \"your_path/lib.176.bin\" # replace with lid path\n",
    "path_to_lid = \"/data/sileo/fasttext/lid.176.bin\"\n",
    "lid = ft.FastText(path_to_lid)\n",
    "\n",
    "def lid_scorer(s, lang=\"en\"):\n",
    "    y=lid.predict_proba([s])[0]\n",
    "    if not y:\n",
    "        return 0.0\n",
    "    y=y[0]\n",
    "    if y[0]!=lang:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return y[1]\n",
    "\n",
    "def matched(s, c1, c2):\n",
    "    count = 0\n",
    "    for i in s:\n",
    "        if i == c1:\n",
    "            count += 1\n",
    "        elif i == c2:\n",
    "            count -= 1\n",
    "        if count < 0:\n",
    "            return False\n",
    "    return count == 0\n",
    "\n",
    "\n",
    "def is_valid(s, lang=\"en\", badchars=\"\", len_range=range(2,32)): \n",
    "    good_len = len(s.split()) in len_range\n",
    "    if not good_len: #avoid computations on long sequences\n",
    "        return False\n",
    "    is_en = lid_scorer(s, lang=lang)>0.75\n",
    "    if not is_en:\n",
    "        return False\n",
    "    letters_prop = (len(re.sub(\"[^a-zA-Z]+\", \"\", s))/len(s))>0.5\n",
    "    parenthesis_balance = matched(s,\"(\",\")\")\n",
    "    quote_balance = s.count('\"')%2==0\n",
    "    no_bad_char = not any(x in s for x in badchars)\n",
    "    has_lowercase = sum(x.islower() for x in s)>sum(x.isupper() for x in s)\n",
    "    has_lowercase = has_lowercase and sum(x.islower() for x in s)>sum(x.istitle() for x in s) \n",
    "    return letters_prop and parenthesis_balance and quote_balance and no_bad_char and has_lowercase and is_en and good_len\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract adverbs or known markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = WordPunctTokenizer().tokenize\n",
    "\n",
    "# initial list of markers; our list of 174 markers can be used instead. \n",
    "pdtb_markers = set(open(os.path.join(data_path, \"pdtb_markers_list\")).read().splitlines())\n",
    "\n",
    "def get_marker(s):\n",
    "    s = s.replace(\" ,\", \",\")\n",
    "    y=[w for w in pdtb_markers if s.lower().startswith(w)]\n",
    "    if y:\n",
    "        return y[0]\n",
    "    else:\n",
    "        return \"\"\n",
    "    \n",
    "# The mining of new markers can be disabled.\n",
    "def get_adverb_or_marker(s):\n",
    "    t_s = tokenizer(s)[:12]\n",
    "    if len(t_s)>2 and nltk.pos_tag(t_s)[0][1] in [\"RB\",] and t_s[1]==\",\" and t_s[0].istitle():\n",
    "        return t_s[0]\n",
    "    else:\n",
    "        return get_marker(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example on depcc corpus\n",
    "depcc is a web corpus  built upon common crawl web data https://www.inf.uni-hamburg.de/en/inst/ab/lt/resources/data/depcc.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, wget, gzip, random\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_path = \"\" #path where you want to download depcc parts\n",
    "corpus_path = \"/data/sileo/libs/Discovery/data/\"\n",
    "\n",
    "def build_l(k):\n",
    "    c=0\n",
    "    d={}\n",
    "    l=[]\n",
    "    filename=f\"part-m-{str(k).zfill(5)}.gz\"\n",
    "\n",
    "    while len(glob.glob(f\"{corpus_path}/*.gz\"))>100:\n",
    "        time.sleep(20)\n",
    "        \n",
    "    print(f\"downloading {k}\")\n",
    "    url = f\"http://ltdata1.informatik.uni-hamburg.de/depcc/corpus/parsed/{filename}\"\n",
    "    wget.download(url, out=corpus_path)\n",
    "    print(f\"downloaded {k}\")\n",
    "    \n",
    "    sentences=[]\n",
    "    s_prev=\"\"\n",
    "    for line in gzip.open(os.path.join(corpus_path, filename), 'rb'):\n",
    "        line = line.decode(\"utf8\")\n",
    "        if line[0]!=\"#\":\n",
    "            continue\n",
    "        c+=1\n",
    "        \n",
    "        tag = line[2:20].split(\" =\")[0]\n",
    "        if tag==\"parser\":\n",
    "            continue\n",
    "        content = line[line.find(\"=\")+2:].strip()\n",
    "        if tag==\"sent_id\":\n",
    "            content=content.split(\"#\")[-1]\n",
    "        if tag==\"text\":\n",
    "            sentences+=[content]\n",
    "\n",
    "        if tag==\"newdoc\turl\":\n",
    "            d[\"doc_id\"]=content\n",
    "            full_text = \" \".join(sentences)\n",
    "            sentences = sent_tokenize(full_text)\n",
    "            for i, pair in enumerate((zip([\"\"]+sentences, sentences))): # iterate over sentence pairs\n",
    "                marker = get_adverb_or_marker(pair[1])\n",
    "                if marker.title() in [\"And\", \"But\"] and random.random()>0.1: # \"And\" and \"But\" are downsampled\n",
    "                    continue\n",
    "                if (marker or random.random()<0.002) and is_valid(pair[0]) and is_valid(pair[1]): # Examples with no adverbs are also downsampled\n",
    "                    d[\"s2\"]=pair[1]\n",
    "                    d[\"s1\"]=pair[0]\n",
    "                    d[\"sent_id\"]=i\n",
    "                    d[\"marker\"]=marker\n",
    "                    l+=[dict(d)]\n",
    "            sentences=[]\n",
    "            s_prev=\"\"\n",
    "    pd.DataFrame(l).to_csv(f\"{corpus_path}/df-{k}.csv\", index=False)\n",
    "    os.remove(f\"{corpus_path}/{filename}\")\n",
    "    print(f\"done {k}\")\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading 1\n",
      "downloading 0\n",
      "downloaded 0\n",
      "downloaded 1\n",
      "done 1\n",
      "downloading 2\n",
      "downloaded 2\n",
      "done 0\n",
      "downloading 3\n",
      "downloaded 3\n",
      "done 2\n",
      "done 3\n"
     ]
    }
   ],
   "source": [
    "n_threads = 2\n",
    "nb_files = 4 # experiment on a small subset of depcc\n",
    "pool = multiprocessing.Pool(n_threads)\n",
    "out = pool.map(build_l, range(0, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove marker at the beginning of s2 and capitalize the resulting sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_depcc_sample = pd.read_csv(f\"{corpus_path}/df-1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_depcc_sample[\"marker\"]=df_depcc_sample[\"marker\"].map(str)\n",
    "df_depcc_sample[\"s2\"] = df_depcc_sample.apply(lambda x: str(x[\"s2\"])[len(x[\"marker\"]):].lstrip(\", \").capitalize(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>marker</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>sent_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://pandasthumb.org/archives/evolution/irre...</td>\n",
       "      <td>Firstly</td>\n",
       "      <td>The more I looked at this sentence, the crasse...</td>\n",
       "      <td>How many male presidents would be defined by t...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://pandasthumb.org/archives/evolution/irre...</td>\n",
       "      <td>Instead</td>\n",
       "      <td>In an ideal world, a cigar-chomping, apoplecti...</td>\n",
       "      <td>The sentence gets syndicated around the world.</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://sportspressnw.com/2144381/2012/whos-the...</td>\n",
       "      <td>So</td>\n",
       "      <td>When you make these kinds of claims in the tee...</td>\n",
       "      <td>Try again.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://sportspressnw.com/2144381/2012/whos-the...</td>\n",
       "      <td>Apparently</td>\n",
       "      <td>I never got to meet him...but now there is You...</td>\n",
       "      <td>Michael behe just doesn't know when to pack it...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://sportspressnw.com/2144381/2012/whos-the...</td>\n",
       "      <td>for example</td>\n",
       "      <td>Note that the unpublished version has a few mi...</td>\n",
       "      <td>It has more emphases which were kind of my way...</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              doc_id       marker  \\\n",
       "0  http://pandasthumb.org/archives/evolution/irre...      Firstly   \n",
       "1  http://pandasthumb.org/archives/evolution/irre...      Instead   \n",
       "2  http://sportspressnw.com/2144381/2012/whos-the...           So   \n",
       "3  http://sportspressnw.com/2144381/2012/whos-the...   Apparently   \n",
       "4  http://sportspressnw.com/2144381/2012/whos-the...  for example   \n",
       "\n",
       "                                                  s1  \\\n",
       "0  The more I looked at this sentence, the crasse...   \n",
       "1  In an ideal world, a cigar-chomping, apoplecti...   \n",
       "2  When you make these kinds of claims in the tee...   \n",
       "3  I never got to meet him...but now there is You...   \n",
       "4  Note that the unpublished version has a few mi...   \n",
       "\n",
       "                                                  s2  sent_id  \n",
       "0  How many male presidents would be defined by t...        2  \n",
       "1     The sentence gets syndicated around the world.        8  \n",
       "2                                         Try again.        2  \n",
       "3  Michael behe just doesn't know when to pack it...       10  \n",
       "4  It has more emphases which were kind of my way...       35  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_depcc_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Yep',\n",
       " 'Superficially',\n",
       " 'Ever',\n",
       " 'Humbly',\n",
       " 'Disturbingly',\n",
       " 'Offensively',\n",
       " 'therefore',\n",
       " 'otherwise,',\n",
       " 'Particularly',\n",
       " 'Acutally']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set(df_depcc_sample.marker))[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
